"""
Simple Graph Implementation - Graph ƒë∆°n gi·∫£n minh h·ªça human-in-the-loop.

Thi·∫øt k·∫ø l·∫°i theo h∆∞·ªõng:
- Kh√¥ng d√πng create_agent / HumanInTheLoopMiddleware (tr√°nh l·ªói get_config).
- V·∫´n gi·ªØ human-in-the-loop ·ªü m·ª©c ·ª©ng d·ª•ng:
  - B∆∞·ªõc 1: LLM ph√¢n lo·∫°i intent (question / request).
  - B∆∞·ªõc 2: N·∫øu request (ghi file), LLM t·ª± ƒë·ªÅ xu·∫•t file_name + file_content.
  - B∆∞·ªõc 3: Tr·∫£ v·ªÅ cho UI ƒë·ªÉ human review (pause) v·ªõi c·ªù __interrupt__.
  - B∆∞·ªõc 4: /continue nh·∫≠n quy·∫øt ƒë·ªãnh c·ªßa human (approve / reject / edit) r·ªìi m·ªõi ghi file.
"""
from typing import Dict, Any, Optional

from langgraph.graph import StateGraph, END

from app.graph.base_graph import BaseGraph
from app.schemas.graph.base import BaseGraphState, IntentClassification, FileInfo
from app.prompts.intent_classification import INTENT_CLASSIFICATION_PROMPT
from app.prompts.extract_file_info import EXTRACT_FILE_INFO_PROMPT

# In-memory store ƒë·ªÉ gi·ªØ th√¥ng tin request theo thread_id gi·ªØa /start v√† /continue.
# Ch·ªâ d√πng cho demo/dev; production n√™n d√πng storage b·ªÅn v·ªØng (DB, Redis, ...).
_PENDING_FILE_REQUESTS: Dict[str, Dict[str, Any]] = {}


class SimpleGraph(BaseGraph):
    """
    SimpleGraph v·ªõi human-in-the-loop ƒë∆∞·ª£c implement ·ªü t·∫ßng ·ª©ng d·ª•ng,
    kh√¥ng ph·ª• thu·ªôc create_agent hay LangChain middleware.

    Flow:
        - question intent: tr·∫£ l·ªùi tr·ª±c ti·∫øp, kh√¥ng c·∫ßn human.
        - request intent (ghi file):
            1) LLM t·∫°o file_name + file_content (ch∆∞a ghi).
            2) Tr·∫£ v·ªÅ UI v·ªõi __interrupt__ v√† waiting_for_human = True.
            3) Human g·ª≠i quy·∫øt ƒë·ªãnh qua /continue:
               - "ƒë·ªìng √Ω"/"approve"  -> ghi file nh∆∞ ƒë·ªÅ xu·∫•t.
               - "t·ª´ ch·ªëi"/"reject"  -> kh√¥ng ghi file.
               - Text kh√°c           -> coi nh∆∞ n·ªôi dung file ƒë√£ ƒë∆∞·ª£c human edit, ghi file v·ªõi n·ªôi dung ƒë√≥.
    """

    def _build_graph(self) -> StateGraph:
        """
        ·ªû b·∫£n thi·∫øt k·∫ø n√†y, ta kh√¥ng d√πng LangGraph cho logic ch√≠nh,
        nh∆∞ng v·∫´n tr·∫£ v·ªÅ m·ªôt graph t·ªëi thi·ªÉu ƒë·ªÉ BaseGraph kh√¥ng l·ªói.
        """
        workflow = StateGraph(BaseGraphState)

        async def _noop(state: BaseGraphState) -> Dict[str, Any]:
            return state

        workflow.add_node("noop", _noop)
        workflow.set_entry_point("noop")
        workflow.add_edge("noop", END)
        return workflow.compile()

    async def _classify_intent(self, query: str) -> str:
        """
        D√πng LLM ƒë·ªÉ ph√¢n lo·∫°i intent (question / request).
        """
        structured_llm = self.llm.with_structured_output(IntentClassification)
        prompt = INTENT_CLASSIFICATION_PROMPT.format(query=query)
        result: IntentClassification = await structured_llm.ainvoke(prompt)
        intent = result.intent.strip().lower()
        if intent not in ("question", "request"):
            # Fail-safe: n·∫øu model tr·∫£ linh tinh, coi nh∆∞ question
            intent = "question"
        return intent

    async def _propose_file(self, query: str) -> FileInfo:
        """
        D√πng LLM ƒë·ªÉ ƒë·ªÅ xu·∫•t file_name + file_content t·ª´ user query.
        """
        structured_llm = self.llm.with_structured_output(FileInfo)
        prompt = EXTRACT_FILE_INFO_PROMPT.format(query=query)
        result: FileInfo = await structured_llm.ainvoke(prompt)
        return result

    async def _answer_question(self, query: str, messages: Optional[list] = None) -> str:
        """
        Tr·∫£ l·ªùi c√¢u h·ªèi b√¨nh th∆∞·ªùng (kh√¥ng c√≥ side-effect).
        """
        from langchain_core.messages import HumanMessage

        history = []
        if messages:
            # Gi·ªØ m·ªçi th·ª© ƒë∆°n gi·∫£n: ch·ªâ d√πng query hi·ªán t·∫°i l√†m input ch√≠nh
            # C√≥ th·ªÉ m·ªü r·ªông ƒë·ªÉ convert full history n·∫øu c·∫ßn.
            pass

        response = await self.llm.ainvoke([HumanMessage(content=query)])
        # ChatOpenAI tr·∫£ v·ªÅ message c√≥ content
        return getattr(response, "content", str(response))

    async def invoke(
        self,
        state: BaseGraphState,
        thread_id: Optional[str] = None,
        resume_value: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Th·ª±c thi SimpleGraph v·ªõi human-in-the-loop ·ªü t·∫ßng ·ª©ng d·ª•ng.

        Args:
            state: Initial state (√≠t nh·∫•t ph·∫£i c√≥ "query" cho l·∫ßn ƒë·∫ßu).
            thread_id: Thread ID ƒë·ªÉ g·∫Øn v·ªõi pending request (b·∫Øt bu·ªôc khi c√≥ human-in-the-loop).
            resume_value: Human input khi resume sau interrupt (approve/reject/edit).
        """
        # Chu·∫©n h√≥a input
        query = state.get("query", "") or ""
        messages = state.get("messages", []) or []
        token_usage = state.get("token_usage", {}) or {}

        # =========================
        # 2. Nh√°nh resume (sau interrupt)
        # =========================
        if resume_value is not None:
            # C·∫ßn c√≥ thread_id ƒë·ªÉ map v·ªÅ pending request
            if not thread_id:
                return {
                    "messages": messages,
                    "query": query,
                    "final_response": "Kh√¥ng c√≥ thread_id ƒë·ªÉ resume human-in-the-loop.",
                    "token_usage": token_usage,
                    "waiting_for_human": False,
                }

            pending = _PENDING_FILE_REQUESTS.get(thread_id)
            if not pending:
                return {
                    "messages": messages,
                    "query": query,
                    "final_response": "Kh√¥ng t√¨m th·∫•y y√™u c·∫ßu ƒëang ch·ªù ph√™ duy·ªát cho thread n√†y.",
                    "token_usage": token_usage,
                    "waiting_for_human": False,
                }

            file_path = pending["file_path"]
            original_content = pending["file_content"]

            decision = str(resume_value).strip().lower()

            # Import tool ghi file
            from app.tools import write_file_tool

            # 3 case:
            # - approve: ƒë·ªìng √Ω / approve
            # - reject : t·ª´ ch·ªëi / reject
            # - edit   : m·ªçi text kh√°c => coi nh∆∞ n·ªôi dung file m·ªõi
            if "ƒë·ªìng √Ω" in decision or "approve" in decision:
                # Ghi file v·ªõi n·ªôi dung g·ªëc do LLM ƒë·ªÅ xu·∫•t
                result_msg = write_file_tool.invoke(
                    {"file_path": file_path, "content": original_content}
                )
                _PENDING_FILE_REQUESTS.pop(thread_id, None)

                final_response = (
                    f"‚úÖ ƒê√£ ghi file theo ƒë·ªÅ xu·∫•t ban ƒë·∫ßu.\n\n"
                    f"File: {file_path}\n\nK·∫øt qu·∫£: {result_msg}"
                )

                return {
                    "messages": messages + [{"role": "assistant", "content": final_response}],
                    "query": query,
                    "final_response": final_response,
                    "token_usage": token_usage,
                    "intent": "request",
                    "file_path": file_path,
                    "file_content": None,  # Kh√¥ng tr·∫£ v·ªÅ content sau khi ƒë√£ ghi
                    "waiting_for_human": False,
                }

            if "t·ª´ ch·ªëi" in decision or "reject" in decision:
                _PENDING_FILE_REQUESTS.pop(thread_id, None)
                final_response = (
                    f"‚ùå B·∫°n ƒë√£ t·ª´ ch·ªëi y√™u c·∫ßu ghi file.\n"
                    f"File ƒë·ªÅ xu·∫•t: {file_path} (KH√îNG ƒë∆∞·ª£c ghi)."
                )
                return {
                    "messages": messages + [{"role": "assistant", "content": final_response}],
                    "query": query,
                    "final_response": final_response,
                    "token_usage": token_usage,
                    "intent": "request",
                    "file_path": file_path,
                    "file_content": None,
                    "waiting_for_human": False,
                }

            # M·ªçi tr∆∞·ªùng h·ª£p kh√°c: coi nh∆∞ n·ªôi dung file ƒë√£ ƒë∆∞·ª£c human edit
            edited_content = str(resume_value)
            result_msg = write_file_tool.invoke(
                {"file_path": file_path, "content": edited_content}
            )
            _PENDING_FILE_REQUESTS.pop(thread_id, None)

            final_response = (
                f"‚úèÔ∏è ƒê√£ ghi file v·ªõi n·ªôi dung b·∫°n cung c·∫•p.\n\n"
                f"File: {file_path}\n\nK·∫øt qu·∫£: {result_msg}"
            )
            return {
                "messages": messages + [{"role": "assistant", "content": final_response}],
                "query": query,
                "final_response": final_response,
                "token_usage": token_usage,
                "intent": "request",
                "file_path": file_path,
                "file_content": None,
                "waiting_for_human": False,
            }

        # =========================
        # 1. L·∫ßn ch·∫°y ƒë·∫ßu (ch∆∞a c√≥ resume_value)
        # =========================
        if not query:
            return {
                "messages": messages,
                "query": "",
                "final_response": "Query r·ªóng, vui l√≤ng nh·∫≠p n·ªôi dung.",
                "token_usage": token_usage,
                "waiting_for_human": False,
            }

        # Ph√¢n lo·∫°i intent
        intent = await self._classify_intent(query)

        # ===== case 1: question -> tr·∫£ l·ªùi tr·ª±c ti·∫øp, kh√¥ng HITL =====
        if intent == "question":
            answer = await self._answer_question(query, messages)
            return {
                "messages": messages + [{"role": "assistant", "content": answer}],
                "query": query,
                "final_response": answer,
                "token_usage": token_usage,
                "intent": "question",
                "waiting_for_human": False,
            }

        # ===== case 2: request -> chu·∫©n b·ªã ghi file, b·∫≠t human-in-the-loop =====
        file_info = await self._propose_file(query)
        file_path = file_info.file_name
        file_content = file_info.file_content

        # L∆∞u pending theo thread_id ƒë·ªÉ l·∫ßn /continue c√≥ th√¥ng tin
        if thread_id:
            _PENDING_FILE_REQUESTS[thread_id] = {
                "file_path": file_path,
                "file_content": file_content,
                "query": query,
            }

        review_message = (
            f"üìù T√¥i ƒë·ªÅ xu·∫•t ghi file sau (CH∆ØA ghi, c·∫ßn b·∫°n duy·ªát):\n\n"
            f"File: {file_path}\n\n"
            f"N·ªôi dung d·ª± ki·∫øn:\n{file_content}\n\n"
            f"H√£y tr·∫£ l·ªùi:\n"
            f"- 'ƒë·ªìng √Ω' ƒë·ªÉ ghi file nh∆∞ tr√™n\n"
            f"- 't·ª´ ch·ªëi' ƒë·ªÉ h·ªßy b·ªè\n"
            f"- Ho·∫∑c nh·∫≠p n·ªôi dung file m·ªõi n·∫øu b·∫°n mu·ªën ch·ªânh s·ª≠a tr∆∞·ªõc khi ghi."
        )

        return {
            "messages": messages + [{"role": "assistant", "content": review_message}],
            "query": query,
            "final_response": review_message,
            "token_usage": token_usage,
            "intent": "request",
            "file_path": file_path,
            "file_content": file_content,
            "waiting_for_human": True,
            "__interrupt__": True,
        }
